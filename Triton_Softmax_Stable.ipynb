{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9BiFCVAMcTDu62TauyCr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soulsharp/Triton_kernels_ViT/blob/main/Triton_Softmax_Stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So3BCCd8C8kG",
        "outputId": "e1a9b153-5371-441a-c0d5-747921a29914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "MbCZPMOWDCzL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APPROACH 1:**\n",
        "\n",
        "Multiple loads from global memory, but no pressure on shared memory\n",
        "\n"
      ],
      "metadata": {
        "id": "oZGZzL3b4t3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _softmax_stable_blocked(\n",
        "    input_ptr,\n",
        "    output_ptr,\n",
        "    input_row_stride,\n",
        "    output_row_stride,\n",
        "    num_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(0)\n",
        "    input_row = input_ptr + pid * input_row_stride\n",
        "    output_row = output_ptr + pid * output_row_stride\n",
        "\n",
        "    # Shared memory arrays for partial reductions\n",
        "    num_blocks = tl.cdiv(num_cols, BLOCK_SIZE)\n",
        "    partial_max = tl.zeros([num_blocks], dtype=tl.float32)\n",
        "    partial_sum = tl.zeros([num_blocks], dtype=tl.float32)\n",
        "\n",
        "    # Computes the row max in blocks\n",
        "    for block_id in range(num_blocks):\n",
        "        start_idx = block_id * BLOCK_SIZE\n",
        "        offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < num_cols\n",
        "        values = tl.load(input_row + offsets, mask=mask, other=0)\n",
        "\n",
        "        # Gets the max of the elements in the block(partial max)\n",
        "        partial_max[block_id] = tl.max(values, axis=0)\n",
        "\n",
        "    # Reduces partial maxima to find the global maximum\n",
        "    row_max = tl.max(partial_max, axis=0)\n",
        "\n",
        "    # Computes the exponentials and partial sums\n",
        "    for block_id in range(num_blocks):\n",
        "        start_idx = block_id * BLOCK_SIZE\n",
        "        offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < num_cols\n",
        "\n",
        "        values = tl.load(input_row + offsets, mask=mask, other=-float('inf'))\n",
        "\n",
        "        # Subtracts row max and exponentiates\n",
        "        values = tl.exp(values - row_max)\n",
        "\n",
        "        # Gets the partial sum of the elements in the  block\n",
        "        partial_sum[block_id] = tl.sum(values, axis=0)\n",
        "\n",
        "        # Writes back intermediate values for normalization\n",
        "        tl.store(output_row + offsets, values, mask=mask)\n",
        "\n",
        "    # Reduces partial sums to find the global sum\n",
        "    row_sum = tl.sum(partial_sum, axis=0)\n",
        "\n",
        "    # Normalizes and writes back the final softmax values\n",
        "    for block_id in range(num_blocks):\n",
        "        start_idx = block_id * BLOCK_SIZE\n",
        "        offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < num_cols\n",
        "\n",
        "        # Loads partially normalized values(x-mean)\n",
        "        values = tl.load(output_row + offsets, mask=mask, other=0)\n",
        "        values /= row_sum\n",
        "\n",
        "        # Writes back the final normalized values\n",
        "        tl.store(output_row + offsets, values, mask=mask)\n"
      ],
      "metadata": {
        "id": "tjdsZOhxDJnG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APPROACH 2 :**\n",
        "\n",
        "Loads each row of the matrix once and performs all computations using\n",
        "shared memory or registers.\n",
        "\n",
        "Becomes shared memory-bound when the size of rows is large."
      ],
      "metadata": {
        "id": "ITh2EKrw5P5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _softmax_stable(\n",
        "    input_ptr,\n",
        "    output_ptr,\n",
        "    input_row_stride,\n",
        "    output_row_stride,\n",
        "    num_cols,\n",
        "    BLOCK_SIZE:tl.constexpr,\n",
        "):\n",
        "\n",
        "  pid = tl.program_id(0)\n",
        "  input_row = input_ptr + pid * input_row_stride\n",
        "  partial_max = tl.zeros([tl.cdiv(num_cols, BLOCK_SIZE)], dtype=tl.float32)\n",
        "  partial_sum = tl.zeros([tl.cdiv(num_cols, BLOCK_SIZE)], dtype=tl.float32)\n",
        "  shared_mem_row = tl.zeros([num_cols], dtype=tl.float32)\n",
        "  partial_max_mem_idx = 0\n",
        "  partial_sum_mem_idx = 0\n",
        "\n",
        "  for offset in range(0, num_cols, BLOCK_SIZE):\n",
        "    offsets = offset + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < num_cols\n",
        "\n",
        "    # row_segment = tl.load(input_row + offsets, mask=mask)\n",
        "    shared_mem_row[offsets] = tl.load(input_row + offsets, mask=mask)\n",
        "    partial_max[partial_max_mem_idx] = tl.max(shared_mem_row, axis=0)\n",
        "    partial_max_mem_idx += 1\n",
        "\n",
        "  row_max = tl.max(partial_max, axis=0)\n",
        "\n",
        "  for offset in range(0, num_cols, BLOCK_SIZE):\n",
        "    offsets = offset + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < num_cols\n",
        "\n",
        "    # row_segment = tl.load(input_row + offsets, mask=mask)\n",
        "    shared_mem_row[offsets] = shared_mem_row[offsets] - row_max\n",
        "    shared_mem_row[offsets] = tl.exp(shared_mem_row[offsets])\n",
        "    partial_sum[partial_sum_mem_idx] = tl.sum(shared_mem_row[offsets], axis=0)\n",
        "    partial_sum_mem_idx += 1\n",
        "\n",
        "  row_sum = tl.sum(partial_sum, axis=0)\n",
        "\n",
        "  for offset in range(0, num_cols, BLOCK_SIZE):\n",
        "    offsets = offset + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < num_cols\n",
        "\n",
        "    shared_mem_row[offsets] /= row_sum\n",
        "\n",
        "    tl.store[output_ptr + offsets, shared_mem_row[offsets], mask]"
      ],
      "metadata": {
        "id": "n0ZNvFe24p6M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _softmax_backward_kernel(\n",
        "    input_ptr,  # ptr to inputs of softmax (Y values)\n",
        "    dldy_ptr,   # DL/DY (derivative of loss wrt softmax outputs)\n",
        "    output_ptr, # DL/DX (derivative of loss wrt logits)\n",
        "    input_row_stride,\n",
        "    output_row_stride,\n",
        "    num_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    input_row = input_ptr + pid * input_row_stride\n",
        "    output_row = output_ptr + pid * output_row_stride\n",
        "    dldy_row = dldy_ptr + pid * input_row_stride\n",
        "    num_blocks = tl.cdiv(num_cols, BLOCK_SIZE)\n",
        "\n",
        "    # Block-wise partial dot product sums\n",
        "    block_dot_sum = tl.zeros([num_blocks], dtype=tl.float32)\n",
        "\n",
        "    # Computes partial dot products\n",
        "    for block_id in range(num_blocks):\n",
        "        start_idx = block_id * BLOCK_SIZE\n",
        "        offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < num_cols\n",
        "\n",
        "        # Loads softmax outputs (Y) and DL/DY\n",
        "        y_values = tl.load(input_row + offsets, mask=mask)\n",
        "        dldy_values = tl.load(dldy_row + offsets, mask=mask)\n",
        "\n",
        "        # Accumulates partial dot product\n",
        "        block_dot_sum[block_id] = tl.sum(y_values * dldy_values, axis=0)\n",
        "\n",
        "    # Reduces dot product across all blocks\n",
        "    final_dot_sum = tl.sum(block_dot_sum, axis=0)\n",
        "\n",
        "    # Computes DL/DX for each block\n",
        "    for block_id in range(num_blocks):\n",
        "        start_idx = block_id * BLOCK_SIZE\n",
        "        offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < num_cols\n",
        "\n",
        "        # Loads softmax outputs (Y) and DL/DY\n",
        "        y_values = tl.load(input_row + offsets, mask=mask)\n",
        "        dldy_values = tl.load(dldy_row + offsets, mask=mask)\n",
        "\n",
        "        # Computes DL/DX gradient and stores them back\n",
        "        dldx_grad = y_values * (dldy_values - final_dot_sum)\n",
        "        tl.store(output_row + offsets, dldx_grad, mask=mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "zB2LgpOv3wiU"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}